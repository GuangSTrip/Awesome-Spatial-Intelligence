<h1 align="center">Awesome-Spatial-Intelligence</h1>

## Introduction

Spatial Intelligence is becoming increasingly important in the field of Artificial Intelligence. This repository aims to provide a comprehensive and systematic collection of research related to Spatial Intelligence.

Any suggestion is welcome, please feel free to raise an issue. ^_^

## Table of Contents

- [Introduction](#introduction)
- [Table of Contents](#table-of-contents)
- [1. Spatial Intelligence in various areas/tasks](#1-spatial-intelligence-in-various-areastasks)
  - [1.1 NLP](#11-nlp)
  - [1.2 CV](#12-cv)
  - [1.3 Multi-modal](#13-multi-modal)
  - [1.4 Others (radar/GPS etc.)](#14-others-radargps-etc)
- [2. Datasets and Benchmarks](#2-datasets-and-benchmarks)
- [3. Spatial Intelligence Methods](#3-spatial-intelligence-methods)
  - [3.1 old/mathematical/rule-based](#31-oldmathematicalrule-based)
  - [3.2 Machine Learning](#32-machine-learning)
  - [3.3 deep learning](#33-deep-learning)
  - [3.4 LLM](#34-llm)
- [Reference Repository](#reference-repository)

## 1. Spatial Intelligence in various areas/tasks

### 1.1 NLP

### 1.2 CV

### 1.3 Multi-modal

### 1.4 Others (radar/GPS etc.)

## 2. Datasets and Benchmarks

**<div style="text-align: center;">2025</div>**

- **Seeing from Another Perspective: Evaluating Multi-View Understanding in MLLMs**  
  [[Paper]](https://arxiv.org/abs/2504.15280)
  [[Project-Page]](https://danielchyeh.github.io/All-Angles-Bench/)
  [[Dataset-All-Angles-Bench]](https://huggingface.co/datasets/ch-chenyu/All-Angles-Bench)

- **Spatial-R1: Enhancing MLLMs in Video Spatial Reasoning**  
  [[Paper]](https://arxiv.org/abs/2504.01805)
  [[Project-Page]](https://github.com/OuyangKun10/Spatial-R1)
  [[Dataset-Spatial-R1-151k]](https://huggingface.co/datasets/RUBBISHLIKE/Spatial-R1-151k)

- **Improved Visual-Spatial Reasoning via R1-Zero-Like Training**  
  [[Paper]](https://arxiv.org/abs/2504.00883)
  [[Project-Page]](https://github.com/zhijie-group/R1-Zero-VSI)

- **From Flatland to Space: Teaching Vision-Language Models to Perceive and Reason in 3D**  
  [[Paper]](https://arxiv.org/pdf/2503.22976)
  [[Project-Page]](https://fudan-zvg.github.io/spar/)
  [[Dataset-SPAR-7M]](https://huggingface.co/datasets/jasonzhango/SPAR-7M)
  [[Dataset-SPAR-Bench]](https://huggingface.co/datasets/jasonzhango/SPAR-Bench)

- **Gemini Robotics: Bringing AI into the Physical World**  
  [[Paper]](https://arxiv.org/abs/2503.20020)
  [[Dataset-PhysicalAI-Robotics-GR00T-X-Embodiment-Sim]](https://huggingface.co/datasets/nvidia/PhysicalAI-Robotics-GR00T-X-Embodiment-Sim)

- **Mind the Gap: Benchmarking Spatial Reasoning in Vision-Language Models**  
  [[Paper]](https://arxiv.org/abs/2503.19707)
  [[Project-Page]](https://github.com/stogiannidis/srbench)
  [[Dataset-srbench]](https://github.com/stogiannidis/srbench)

- **Open3DVQA: A Benchmark for Comprehensive Spatial Reasoning with Multimodal Large Language Model in Open Space**  
  [[Paper]](https://www.arxiv.org/abs/2503.11094)
  [[Project-Page]](https://github.com/WeichenZh/Open3DVQA)
  [[Dataset-Open3DVQA]](https://github.com/WeichenZh/Open3DVQA)

- **Spatial457: A Diagnostic Benchmark for 6D Spatial Reasoning of Large Multimodal Models**  
  [[Paper]](https://arxiv.org/abs/2502.08636)
  [[Project-Page]](https://xingruiwang.github.io/projects/Spatial457/)
  [[Dataset-Spatial457]](https://huggingface.co/datasets/RyanWW/Spatial457)

- **iVISPAR â€” An Interactive Visual-Spatial Reasoning Benchmark for VLMs**  
  [[Paper]](https://arxiv.org/abs/2502.03214)
  [[Project-Page]](https://ivispar.ai/)
  [[Dataset-iVISPAR]](https://github.com/SharkyBamboozle/iVISPAR)

- **SoFar: Language-Grounded Orientation Bridges Spatial Reasoning and Object Manipulation**  
  [[Paper]](https://arxiv.org/abs/2502.13143)
  [[Project-Page]](https://qizekun.github.io/sofar/)
  [[Dataset-OrienText300K]](https://huggingface.co/datasets/qizekun/OrienText300K)
  [[Dataset-6DoF-SpatialBench]](https://huggingface.co/datasets/qizekun/6DoF-SpatialBench)

- **PhysBench: Benchmarking and Enhancing Vision-Language Models for Physical World Understanding**  
  [[Paper]](https://arxiv.org/abs/2501.16411)
  [[Project-Page]](https://physbench.github.io/)
  [[Dataset-PhysBench]](https://huggingface.co/datasets/USC-GVL/PhysBench)

---
**<div style="text-align: center;">2024</div>**

- **[Do Multimodal Language Models Really Understand Direction? A Benchmark for Compass Direction Reasoning](https://arxiv.org/abs/2412.16599)**  

- **[Thinking in Space: How Multimodal Large Language Models See, Remember, and Recall Spaces](https://arxiv.org/abs/2412.14171)**  
  [[Project-Page]](https://vision-x-nyu.github.io/thinking-in-space.github.io/)
  [[Dataset-VSI-Bench]](https://huggingface.co/datasets/nyu-visionx/VSI-Bench)

- **[SPHERE: Unveiling Spatial Blind Spots in Vision-Language Models Through Hierarchical Evaluation](https://arxiv.org/abs/2412.12693)**  
  [[Project-Page]](https://github.com/zwenyu/SPHERE-VLM)
  [[Dataset-SPHERE-VLM]](https://huggingface.co/datasets/wei2912/SPHERE-VLM)

- **[Stereo4D: Learning How Things Move in 3D from Internet Stereo Videos](https://arxiv.org/abs/2412.09621)**  
  [[Project-Page]](https://stereo4d.github.io/)
  [[Dataset-Stereo4D]](https://console.cloud.google.com/storage/browser/stereo4d/)

- **[3DSRBench: A Comprehensive 3D Spatial Reasoning Benchmark](https://arxiv.org/abs/2412.07825)**  
  [[Project-Page]](https://3dsrbench.github.io/)
  [[Dataset-3DSRBench]](https://huggingface.co/datasets/ccvl/3DSRBench)

- **[SAT: Dynamic Spatial Aptitude Training for Multimodal Language Models](https://arxiv.org/abs/2412.07755)**  
  [[Project-Page]](https://arijitray.com/SAT/)
  [[Dataset-SAT]](https://huggingface.co/datasets/array/SAT)

- **[ProVision: Programmatically Scaling Vision-centric Instruction Data for Multimodal Language Models](https://arxiv.org/abs/2412.07012)**  
  [[Project-Page]](https://github.com/JieyuZ2/ProVision)
  [[Dataset-ProVision-10M]](https://huggingface.co/datasets/Salesforce/ProVision-10M)

- **[Video-3D LLM: Learning Position-Aware Video Representation for 3D Scene Understanding](https://arxiv.org/abs/2412.00493)**  
  [[Project-Page]](https://github.com/LaVi-Lab/Video-3D-LLM)
  [[Dataset-Video-3D-LLM_data]](https://huggingface.co/datasets/zd11024/Video-3D-LLM_data)

- **[RoboSpatial: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics](https://arxiv.org/abs/2411.16537)**  
  [[Project-Page]](https://chanh.ee/RoboSpatial/)
  [[Dataset-RoboSpatial-Home]](https://huggingface.co/datasets/chanhee-luke/RoboSpatial-Home)

- **[An Empirical Analysis on Spatial Reasoning Capabilities of Large Multimodal Models](https://arxiv.org/abs/2411.06048)**  
  [[Project-Page]](https://github.com/FatemehShiri/Spatial-MM)
  [[Dataset-Spatial-MM]](https://github.com/FatemehShiri/Spatial-MM)

- **[Do Vision-Language Models Represent Space and How? Evaluating Spatial Frame of Reference Under Ambiguities](https://arxiv.org/abs/2410.17385)**  
  [[Project-Page]](https://spatial-comfort.github.io/)
  [[Dataset-COMFORT]](https://huggingface.co/datasets/sled-umich/COMFORT)

- **[MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks](https://arxiv.org/abs/2410.10563)**  
  [[Project-Page]](https://tiger-ai-lab.github.io/MEGA-Bench/)
  [[Dataset-MEGA-Bench]](https://huggingface.co/datasets/TIGER-Lab/MEGA-Bench)

- **[Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Vision-Language Models](https://arxiv.org/abs/2409.17146)**  
  [[Project-Page]](https://allenai.org/blog/molmo)
  [[Dataset-PixMo]](https://huggingface.co/collections/allenai/pixmo-674746ea613028006285687b)

- **[Can Vision Language Models Learn from Visual Demonstrations of Ambiguous Spatial Reasoning?](https://arxiv.org/abs/2409.17080)**  
  [[Project-Page]](https://github.com/groundlight/vlm-visual-demonstrations)

- **[Reasoning Paths with Reference Objects Elicit Quantitative Spatial Reasoning in Large Vision-Language Models](https://arxiv.org/abs/2409.09788)**  
  [[Project-Page]](https://andrewliao11.github.io/spatial_prompt/)
  [[Dataset-Q-Spatial-Bench]](https://huggingface.co/datasets/andrewliao11/Q-Spatial-Bench)

- **[GRASP: A Grid-Based Benchmark for Evaluating Commonsense Spatial Reasoning](https://arxiv.org/abs/2407.01892)**  
  [[Project-Page]](https://github.com/jasontangzs0/GRASP)
  [[Dataset-GRASP]](https://github.com/jasontangzs0/GRASP)

- **[Cambrian-1: Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs](https://arxiv.org/abs/2406.16860)**  
  [[Project-Page]](https://cambrian-mllm.github.io/)
  [[Dataset-CV-Bench]](https://huggingface.co/datasets/nyu-visionx/CV-Bench)
  [[Dataset-Cambrian-10M]](https://huggingface.co/datasets/nyu-visionx/Cambrian-10M)

- **[Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models](https://arxiv.org/abs/2406.14852)**  
  [[Project-Page]](https://spatialeval.github.io/)
  [[Dataset-SpatialEval]](https://huggingface.co/datasets/MilaWang/SpatialEval)

- **[GSR-Bench: A Benchmark for Grounded Spatial Reasoning Evaluation via Multimodal LLMs](https://arxiv.org/abs/2406.13246)**  

- **[RoboPoint: A Vision-Language Model for Spatial Affordance Prediction for Robotics](https://arxiv.org/abs/2406.10721)**  
  [[Project-Page]](https://robo-point.github.io/)
  [[Dataset-ReboPoint-Data]](https://huggingface.co/datasets/wentao-yuan/robopoint-data)

- **[ImageNet3D: Towards General-Purpose Object-Level 3D Understanding](https://arxiv.org/abs/2406.09613)**  
  [[Project-Page]](https://imagenet3d.github.io/)
  [[Dataset-ImageNet3D]](https://huggingface.co/datasets/ccvl/ImageNet3D)


- **[EmbSpatial-Bench: Benchmarking Spatial Understanding for Embodied Tasks with Large Vision-Language Models](https://arxiv.org/abs/2406.05756)**  
  [[Project-Page]](https://github.com/mengfeidu/EmbSpatial-Bench)
  [[Dataset-EmbSpatial-Bench]](https://huggingface.co/datasets/Phineas476/EmbSpatial-Bench)

- **[SpaRC and SpaRP: Spatial Reasoning Characterization and Path Generation for Understanding Spatial Reasoning Capability of Large Language Models](https://arxiv.org/abs/2406.04566)**  
  [[Project-Page]](https://github.com/UKPLab/acl2024-sparc-and-sparp)
  [[Dataset-sparp]](https://huggingface.co/datasets/UKPLab/sparp)

- **[TopViewRS: Vision-Language Models as Top-View Spatial Reasoners](https://arxiv.org/abs/2406.02537)**  
  [[Project-Page]](https://topviewrs.github.io/)
  [[Dataset-topviewrs]](https://huggingface.co/datasets/chengzu/topviewrs)

- **[SpatialRGPT: Grounded Spatial Reasoning in Vision Language Models](https://arxiv.org/abs/2406.01584)**  
  [[Project-Page]](https://www.anjiecheng.me/SpatialRGPT)
  [[Dataset-SpatialRGPT-Bench]](https://huggingface.co/datasets/a8cheng/SpatialRGPT-Bench)
  [Dataset-OpenSpatialDataset](https://huggingface.co/datasets/a8cheng/OpenSpatialDataset)

- **[Compositional 4D Dynamic Scenes Understanding with Physics Priors for Video Question Answering](https://arxiv.org/abs/2406.00622)**  
  [[Project-Page]](https://xingruiwang.github.io/projects/DynSuperCLEVR/)
  [[Dataset-DynSuperCLEVR]](https://github.com/XingruiWang/DynSuperCLEVR)

- **[Reframing Spatial Reasoning Evaluation in Language Models: A Real-World Simulation Benchmark for Qualitative Reasoning](https://arxiv.org/abs/2405.15064)**  
  [[Project-Page]](https://github.com/Fangjun-Li/RoomSpace)
  [[Dataset-RoomSpace]](https://huggingface.co/datasets/Fangjun/RoomSpace)
  
- **[BLINK: Multimodal Large Language Models Can See but Not Perceive](https://arxiv.org/abs/2404.12390)**  
  [[Project-Page]](https://zeyofu.github.io/blink/)
  [[Dataset-BLINK]](https://huggingface.co/datasets/BLINK-Benchmark/BLINK)

- **[Proximity QA: Unleashing the Power of Multi-Modal Large Language Models for Spatial Proximity Analysis](https://arxiv.org/abs/2401.17862)**  
  [[Project-Page]](https://github.com/NorthSummer/ProximityQA)
  [[Dataset-ProximityQA]](https://huggingface.co/Electronics/ProximityQA)

- **[R2D3:ImpartingSpatial Reasoning by Reconstructing 3D Scenes from 2D Images](https://openreview.net/pdf?id=Ku4lylDpjq)**  
  [[Dataset-r2d3_data]](https://huggingface.co/datasets/array/r2d3_data)

- **[Holistic Autonomous Driving Understanding by Bird's-Eye-View Injected Multi-Modal Large Models](https://arxiv.org/abs/2401.00988)**  
  [[Project-Page]](https://github.com/xmed-lab/NuInstruct)
  [[Dataset-NuInstruct]](https://github.com/xmed-lab/NuInstruct)

---
**<div style="text-align: center;">2023</div>**

- **[3D-Aware Visual Question Answering about Parts, Poses and Occlusions](https://arxiv.org/abs/2310.17914)**  
  [[Project-Page]](https://github.com/XingruiWang/3D-Aware-VQA)
  [[Dataset-superclevr-3D-question]](https://github.com/XingruiWang/superclevr-3D-question)

- **[Open X-Embodiment: Robotic Learning Datasets and RT-X Models](https://arxiv.org/abs/2310.08864)**  
  [[Project-Page]](https://robotics-transformer-x.github.io/)
  [[Dataset-Open X-Embodiment]](https://docs.google.com/spreadsheets/d/1rPBD77tk60AEIGZrGSODwyyzs5FgCU9Uz3h-3_t2A9g/edit?gid=0#gid=0)

---
**<div style="text-align: center;">2022</div>**

- **[Things not Written in Text: Exploring Spatial Commonsense from Visual Signals](https://arxiv.org/abs/2203.08075)**  
  [[Project-Page]](https://github.com/xxxiaol/spatial-commonsense)
  [[Dataset-spatial-commonsense]](https://github.com/xxxiaol/spatial-commonsense)

---
**<div style="text-align: center;">2020</div>**

- **[SPARE3D: A Dataset for SPAtial REasoning on Three-View Line Drawings](https://arxiv.org/abs/2003.14034)**  
  [[Project-Page]](https://github.com/ai4ce/SPARE3D)
  [[Dataset-SPARE3D]](https://drive.google.com/drive/folders/1Mi2KZyKAlUOGYRQTDz3E5nhiXY5GhUB2)

## 3. Spatial Intelligence Methods

### 3.1 old/mathematical/rule-based

### 3.2 Machine Learning

### 3.3 deep learning

### 3.4 LLM

## Reference Repository

- [Awesome-Spatial-Reasoning](https://github.com/yyyybq/Awesome-Spatial-Reasoning)
- [awesome-spatial-reasoning](https://github.com/arijitray1993/awesome-spatial-reasoning)
- [awesome-3d-spatial-reasoning](https://github.com/wufeim/awesome-3d-spatial-reasoning)
- [Awesome-Spatial-Intelligence (for Robotics)](https://github.com/lif314/Awesome-Spatial-Intelligence)
- [Awesome-Visual-Spatial-Intelligence](https://github.com/bobochow/Awesome-Visual-Spatial-Intelligence)
- [Awesome-VLA-Robotics](https://github.com/Jiaaqiliu/Awesome-VLA-Robotics)
